import pandas as pd
from os import path

configfile: "config.yml"

manifest = pd.read_csv(path.join(config["data"], config["manifest"]))

def str_to_taxa(taxon):
    taxon = taxon.split("|")
    taxa = pd.Series({t.split("__")[0]: t.split("__")[1] for t in taxon})
    return taxa

def summarize_rank(counts, level):
    lidx = counts.columns.get_loc(level)
    valid = counts.iloc[:, lidx].notnull()
    valid &= counts.iloc[:, (lidx+1):8].apply(
        lambda s: pd.isnull(s).all(),
        axis=1)
    summarized = (
        counts[valid].groupby(counts.columns[0:(lidx + 1)].
        tolist() + ["sample"]).reads.sum().reset_index()
    )
    return summarized 

rule all:
    input:
        expand(
            path.join(config["data"], "{lev}_counts.csv"), 
            lev=["P", "G", "S"]
        )

rule preprocess:
    input:
        path.join(config["data"], "raw", "{sample}.fastq.gz")
    output:
        path.join(config["data"], "preprocessed", "{sample}.fastq.gz"),
        path.join(config["data"], "preprocessed", "{sample}_report.json"),
        path.join(config["data"], "preprocessed", "{sample}_report.html")
    params:
        trim_front = 0,
        min_length = 15,
        quality_threshold = 20
    threads: 1
    shell:
        "fastp -i {input[0]} -o {output[0]} "
        "--json {output[1]} --html {output[2]} "
        "--trim_front1 {params.trim_front} -l {params.min_length} "
        "-3 -M {params.quality_threshold} -r {input[0]}"

rule classify:
    input:
        path.join(config["data"], "preprocessed", "{sample}.fastq.gz")
    output:
        path.join(config["data"], "taxonomy", "{sample}.k2"),
        path.join(config["data"], "taxonomy", "{sample}.tsv")
    threads: 4
    shell:
        "kraken2 --db /proj/gibbons/refs/kraken2_default "
        "--threads {threads} --gzip-compressed --output {output[0]} "
        " --report {output[1]} {input}"

rule count:
    input:
        path.join(config["data"], "taxonomy", "{sample}.tsv")
    output:
        path.join(config["data"], "taxonomy", "{level}_{sample}.b2")
    params:
        threshold = 10,
        read_length = 100
    threads: 1
    shell:
        "bracken -d /proj/gibbons/refs/kraken2_default -i {input} "
        "-l {wildcards.level} -o {output} -r {params.read_length} -t {params.threshold}" 

rule merge:
    input:
        expand(
            path.join(config["data"], "taxonomy", "{{level}}_{sample}.b2"),
            sample=manifest.id
        )
    output:
        path.join(config["data"], "{level}_counts.csv")
    threads: 1
    run:
        taxa_files = []
        for sa in manifest.id:
            counts = pd.read_csv(
                path.join(config["data"], "taxonomy", "%s_%s.b2" % (wildcards.level, sa)), 
                sep="\t"
            )
            counts.columns = ["name", "taxid", "rank", "raw_reads", "added_reads", "corrected_reads", "corrected_relative"]
            counts["sample"] = sa
            taxa_files.append(counts)
        pd.concat(taxa_files).to_csv(str(output))

