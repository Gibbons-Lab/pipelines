import pandas as pd
from os import path
from loguru import logger as loggy


configfile: "config.yml"

manifest = pd.read_csv(path.join(config["data"], "manifest.csv"))
samples = manifest.iloc[:, 1].str.split("_R\\d_001.+$").str[0]

wildcard_constraints:
    sample="[^/]+"

rule all:
    input:
        path.join(config["data"], "function_counts.csv.gz")

rule preprocess:
    input:
        path.join(config["data"], "raw", "{sample}_R1_001.fastq.gz"),
        path.join(config["data"], "raw", "{sample}_R2_001.fastq.gz")
    output:
        path.join(config["data"], "preprocessed", "{sample}_R1_001.fastq.gz"),
        path.join(config["data"], "preprocessed", "{sample}_R2_001.fastq.gz"),
        path.join(config["data"], "preprocessed", "{sample}_report.json"),
        path.join(config["data"], "preprocessed", "{sample}_report.html")
    params:
        trim_front = 5,
        min_length = 15,
        quality_threshold = 20
    threads: 1
    shell:
        "fastp -i {input[0]} -I {input[1]} -o {output[0]} -O {output[1]} "
        "--json {output[2]} --html {output[3]} "
        "--trim_front1 {params.trim_front} -l {params.min_length} "
        "-3 -M {params.quality_threshold} -r {input[0]}"

rule assemble:
    input:
        expand(
            path.join(config["data"], "preprocessed", "{s}_R1_001.fastq.gz"),
            s=samples
        ),
        expand(
            path.join(config["data"], "preprocessed", "{s}_R2_001.fastq.gz"),
            s=samples
        )
    output:
        path.join(config["data"], "assembly", "final.contigs.fa")
    params:
        forward = ",".join(
            path.join(config["data"], "preprocessed", s + "_R1_001.fastq.gz") for
            s in samples
            ),
        reverse = ",".join(
            path.join(config["data"], "preprocessed", s + "_R2_001.fastq.gz") for
            s in samples
            ),
        outdir = path.join(config["data"], "assembly")
    threads: 16
    shell:
        "rm -rf {params.outdir} && "
        "megahit -1 {params.forward} -2 {params.reverse} "
        "-o {params.outdir} -t {threads}"

rule assembly_align:
    input:
        path.join(config["data"], "preprocessed", "{sample}_R1_001.fastq.gz"),
        path.join(config["data"], "preprocessed", "{sample}_R2_001.fastq.gz"),
        path.join(config["data"], "assembly", "final.contigs.fa")
    output:
        path.join(config["data"], "align", "{sample}.bam"),
        path.join(config["data"], "align", "{sample}.bai")
    threads: 16
    shell:
        "minimap2 -ax sr -t {threads} {input[2]} {input[0]} {input[1]} | "
        "samtools sort -@{threads} -o {output[0]} && "
        "samtools index {output[0]} {output[1]}"


rule binning:
    input:
        path.join(config["data"], "assembly", "final.contigs.fa"),
        expand(
            path.join(config["data"], "align", "{s}.bam"),
            s=samples
        )
    output:
        path.join(config["data"], "binning", "depth.txt")
    params:
        bins = path.join(config["data"], "binning")
    threads: 16
    run:
        if len(input) == 2:
            files = [input[1]]
        else:
            files = input[1:len(input)]
        shell(
            "jgi_summarize_bam_contig_depths --outputDepth {output} " +
            " ".join(files)
        )
        shell(
            "metabat2 -t {threads} -i {input[0]} -a {output} " +
            "-o {params.bins}/bin"
        )

rule checkm:
    input:
        path.join(config["data"], "binning", "depth.txt")
    output:
        path.join(config["data"], "binning", "checkm", "checkm.txt")
    threads: 16
    params:
        bins = path.join(config["data"], "binning"),
        outdir = path.join(config["data"], "binning", "checkm")
    conda:
        "checkm.yml"
    shell:
        "checkm data setRoot data/checkm && "
        "checkm lineage_wf -t {threads} -x fa {params.bins} {params.outdir} "
        "-f {output} --tab_table"

rule find_genes:
    input:
        path.join(config["data"], "assembly", "final.contigs.fa")
    output:
        path.join(config["data"], "annotate", "metagenome.gff"),
        path.join(config["data"], "annotate", "metagenome.ffn"),
        path.join(config["data"], "annotate", "metagenome.faa")
    params:
        outdir = path.join(config["data"], "annotate")
    threads: 1
    shell:
        "prodigal -p meta -i {input} -o {output[0]} -d {output[1]} "
        "-a {output[2]} -q"

rule transcript_align:
    input:
        path.join(config["data"], "annotate", "metagenome.ffn"),
        path.join(config["data"], "preprocessed", "{sample}_R1_001.fastq.gz"),
        path.join(config["data"], "preprocessed", "{sample}_R2_001.fastq.gz")
    output:
        path.join(config["data"], "txn_align", "{sample}.bam")
    shell:
        "minimap2 -acx sr -t {threads} {input[0]} {input[1]} {input[2]} | "
        "samtools view -bS - > {output}"

rule em_count:
    input:
        path.join(config["data"], "annotate", "metagenome.ffn"),
        path.join(config["data"], "txn_align", "{sample}.bam")
    output:
        path.join(config["data"], "em_count", "{sample}/quant.sf")
    params:
        outdir = path.join(config["data"], "em_count", "{sample}")
    threads: 16
    shell:
        "salmon quant -p {threads} -l SF "
        "-t {input[0]} -a {input[1]} -o {params.outdir} &> /dev/null"

rule merge:
    input:
        expand(
            path.join(config["data"], "em_count", "{s}", "quant.sf"),
            s = samples
        )
    output:
        path.join(config["data"], "function_counts.csv.gz")
    run:
        read = []
        paths = input
        if len(input) == 1:
            paths = [input]
        for p in paths:
            sample = path.basename(path.dirname(p))
            loggy.info("Processing sample {}...", sample)
            counts = pd.read_csv(p, sep="\t").query("NumReads > 0.1")
            counts.columns = [
                "locus_tag", "length", "effective_length", "tpm", "reads"]
            counts["sample"] = sample
            read.append(counts)
        read = pd.concat(read)
        loggy.info("writing compressed output")
        read.to_csv(output[0])

